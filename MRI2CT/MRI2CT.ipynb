{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import glob\n",
    "from glob import glob\n",
    "import nibabel as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import glob\n",
    "from glob import glob\n",
    "import nibabel as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code will run on GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    bs : int\n",
    "    n_channels : int\n",
    "    ngf : int\n",
    "    ndf : int\n",
    "    size : int\n",
    "    gen_n_down : int\n",
    "    gen_n_blocks : int\n",
    "    dis_n_down : int\n",
    "    lr : float\n",
    "    beta1 : float\n",
    "    beta1_disc : float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_brain(img):\n",
    "    img = np.array(img)\n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "    \n",
    "    low_value = 0\n",
    "    high_value = 0.45\n",
    "\n",
    "    img[0:128,0:128][(img[0:128, 0:128] >= low_value) & (img[0:128,0:128] <= high_value)] = 0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataloaders(p):\n",
    "    workers = 2\n",
    "    image_size = (64,64)\n",
    "    dataroot = r'C:\\Users\\Xiaowei\\Desktop\\Clara\\CycleGAN\\Datasets\\CT_dataset -y3_ Copy' \n",
    "    datasets_train = dset.ImageFolder(root=dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(image_size),\n",
    "                                   transforms.CenterCrop(image_size),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0, 0, 0), (1, 1, 1)),\n",
    "                                  ]))\n",
    "\n",
    "    from torch.utils import data\n",
    "    idx = [i for i in range(len(datasets_train)) if datasets_train.imgs[i][1] != datasets_train.class_to_idx['B']]\n",
    "    mri_subset = data.Subset(datasets_train, idx)\n",
    "    dataloader_mri = torch.utils.data.DataLoader(mri_subset, batch_size=p.bs,\n",
    "                                             shuffle=True, num_workers=workers)\n",
    "    idx = [i for i in range(len(datasets_train)) if datasets_train.imgs[i][1] != datasets_train.class_to_idx['A']]\n",
    "    sos_subset = data.Subset(datasets_train, idx)\n",
    "\n",
    "    dataloader_sos = torch.utils.data.DataLoader(sos_subset, batch_size=p.bs,\n",
    "                                             shuffle=True, num_workers=workers)\n",
    "    return dataloader_mri, dataloader_sos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResnetGenerator import ResnetGenerator\n",
    "from NLayerDiscriminator import NLayerDiscriminator\n",
    "\n",
    "def LSGAN_D(real, fake):\n",
    "  return (torch.mean((real - 1)**2) + torch.mean(fake**2))\n",
    "\n",
    "def LSGAN_G(fake):\n",
    "  return  torch.mean((fake - 1)**2)\n",
    "\n",
    "criterion_Im = torch.nn.L1Loss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models(p):\n",
    "    G_A2B = ResnetGenerator(input_nc=p.n_channels,output_nc=p.n_channels,ngf=p.ngf,norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=p.gen_n_blocks, n_downsampling=p.gen_n_down, padding_type='reflect')\n",
    "    G_B2A = ResnetGenerator(input_nc=p.n_channels,output_nc=p.n_channels,ngf=p.ngf,norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=p.gen_n_blocks, padding_type='reflect')\n",
    "    D_A = NLayerDiscriminator(input_nc=p.n_channels,ndf=p.ndf,n_layers=p.dis_n_down, norm_layer=nn.BatchNorm2d)\n",
    "    D_B = NLayerDiscriminator(input_nc=p.n_channels,ndf=p.ndf,n_layers=p.dis_n_down, norm_layer=nn.BatchNorm2d)\n",
    "\n",
    "    optimizer_G_A2B = torch.optim.Adam(G_A2B.parameters(), lr=p.lr, betas=(p.beta1, 0.999))\n",
    "    optimizer_G_B2A = torch.optim.Adam(G_B2A.parameters(), lr=p.lr, betas=(p.beta1, 0.999))\n",
    "\n",
    "    optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=p.lr, betas=(p.beta1_disc, 0.999))\n",
    "    optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=p.lr, betas=(p.beta1_disc, 0.999))\n",
    "\n",
    "    return G_A2B, G_B2A, D_A, D_B, optimizer_G_A2B, optimizer_G_B2A, optimizer_D_A, optimizer_D_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(G_A2B, G_B2A, D_A, D_B, name):\n",
    "\n",
    "  torch.save(G_A2B, name+\"_G_A2B.pt\")\n",
    "  torch.save(G_B2A,  name+\"_G_B2A.pt\")\n",
    "  torch.save(D_A,  name+\"_D_A.pt\")\n",
    "  torch.save(D_B, name+\"_D_B.pt\")\n",
    "\n",
    "def load_models( name):\n",
    "  G_A2B=torch.load(name+'_G_A2B.pt', map_location=torch.device('cpu'))\n",
    "  G_B2A=torch.load(name+\"_G_B2A.pt\",map_location=torch.device('cpu'))\n",
    "  D_A=torch.load(name+\"_D_A.pt\", map_location=torch.device('cpu'))\n",
    "  D_B=torch.load(name+\"_D_B.pt\", map_location=torch.device('cpu'))\n",
    "  return G_A2B, G_B2A, D_A, D_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep(p,name):\n",
    "\n",
    "    dataloader_mri, dataloader_sos = init_dataloaders(p)\n",
    "\n",
    "    G_A2B, G_B2A, D_A, D_B, optimizer_G_A2B, optimizer_G_B2A, optimizer_D_A, optimizer_D_B = init_models(p)\n",
    "    G_A2B.to(device)\n",
    "    G_B2A.to(device)\n",
    "    D_A.to(device)\n",
    "    D_B.to(device)\n",
    "\n",
    "    G_losses, D_losses = training(100, dataloader_mri, dataloader_sos, G_A2B, G_B2A, D_A, D_B, optimizer_G_A2B, optimizer_G_B2A, optimizer_D_A, optimizer_D_B,name)\n",
    "#     print(\"Generator loss at the beginning of each epoch \",G_losses[::1653])\n",
    "#     print(\"Generator minimal loss \",min(G_losses), \"iteration \",G_losses.index(min(G_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep(p,name):\n",
    "\n",
    "    dataloader_mri, dataloader_sos = init_dataloaders(p)\n",
    "\n",
    "    G_A2B, G_B2A, D_A, D_B= load_models(\"4\"+name)\n",
    "    G_A2B.to(device)\n",
    "    G_B2A.to(device)\n",
    "    D_A.to(device)\n",
    "    D_B.to(device)\n",
    "    optimizer_G_A2B = torch.optim.Adam(G_A2B.parameters(), lr=p.lr, betas=(p.beta1, 0.999))\n",
    "    optimizer_G_B2A = torch.optim.Adam(G_B2A.parameters(), lr=p.lr, betas=(p.beta1, 0.999))\n",
    "\n",
    "    optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=p.lr, betas=(p.beta1_disc, 0.999))\n",
    "    optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=p.lr, betas=(p.beta1_disc, 0.999))\n",
    "\n",
    "    G_losses, D_losses = training(3, dataloader_mri, dataloader_sos, G_A2B, G_B2A, D_A, D_B, optimizer_G_A2B, optimizer_G_B2A, optimizer_D_A, optimizer_D_B,name)\n",
    "#     print(\"Generator loss at the beginning of each epoch \",G_losses[::1653])\n",
    "#     print(\"Generator minimal loss \",min(G_losses), \"iteration \",G_losses.index(min(G_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs, dataloader_mri, dataloader_sos, G_A2B, G_B2A, D_A, D_B, optimizer_G_A2B, optimizer_G_B2A, optimizer_D_A, optimizer_D_B,name):\n",
    "    \n",
    "    iters=0\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    print('Starting training loop')\n",
    "    # For each epoch\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # For each batch in the dataloader\n",
    "        for i,(data_mri, data_sos) in enumerate(zip(dataloader_mri, dataloader_sos),0):\n",
    "            temp_D_losses = 0\n",
    "            # print(b)\n",
    "            # Set model input\n",
    "            a_real = data_mri[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "            b_real = data_sos[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "            \n",
    "            # tensor_ones=torch.ones([a_real.shape[0],1,14,14]).to(device)\n",
    "            # tensor_zeros=torch.zeros([a_real.shape[0],1,14,14]).to(device)\n",
    "\n",
    "            # Generated images\n",
    "            # print(\"a_real shape: \"+str(a_real.shape))\n",
    "            b_fake = G_A2B(a_real)\n",
    "            # print(\"fake image generated\")\n",
    "            a_rec = G_B2A(b_fake)\n",
    "            a_fake = G_B2A(b_real)\n",
    "            b_rec = G_A2B(a_fake)\n",
    "\n",
    "            # CALCULATE DISCRIMINATORS LOSSES\n",
    "            # Discriminator A\n",
    "            optimizer_D_A.zero_grad()\n",
    "            if((iters > 0 or epoch > 0) and iters % 3 == 0):\n",
    "                rand_int = random.randint(1, old_a_fake.shape[0]-1)\n",
    "                Disc_loss_A = LSGAN_D(D_A(a_real), D_A(old_a_fake[rand_int-1:rand_int].detach()))\n",
    "                temp_D_losses += Disc_loss_A\n",
    "\n",
    "            else:\n",
    "                Disc_loss_A = LSGAN_D(D_A(a_real), D_A(a_fake.detach()))\n",
    "                temp_D_losses += Disc_loss_A\n",
    "\n",
    "            # Disc_loss_A = LSGAN_D(D_A(a_real), D_A(a_fake.detach()))\n",
    "            # D_A_losses.append(Disc_loss_A.item())\n",
    "            \n",
    "            Disc_loss_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            \n",
    "            # Discriminator B\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "            if((iters > 0 or epoch > 0) and iters % 3 == 0):\n",
    "                rand_int = random.randint(1, old_b_fake.shape[0]-1)\n",
    "                Disc_loss_B =  LSGAN_D(D_B(b_real), D_B(old_b_fake[rand_int-1:rand_int].detach()))\n",
    "                temp_D_losses += Disc_loss_B\n",
    "            else:\n",
    "                Disc_loss_B =  LSGAN_D(D_B(b_real), D_B(b_fake.detach()))\n",
    "                temp_D_losses += Disc_loss_B\n",
    "\n",
    "            Disc_loss_B.backward()\n",
    "            optimizer_D_B.step()   \n",
    "            \n",
    "            D_losses.append(temp_D_losses) # different\n",
    "            # Generator\n",
    "            \n",
    "\n",
    "            optimizer_G_A2B.zero_grad()\n",
    "            optimizer_G_B2A.zero_grad()\n",
    "\n",
    "\n",
    "            # CALCULATE GENERATORS LOSSES\n",
    "            Fool_disc_loss_A2B = LSGAN_G(D_B(b_fake))\n",
    "            Fool_disc_loss_B2A = LSGAN_G(D_A(a_fake))\n",
    "\n",
    "            # Cycle Consistency    both use the two generators\n",
    "            Cycle_loss_A = criterion_Im(a_rec, a_real)*5\n",
    "            Cycle_loss_B = criterion_Im(b_rec, b_real)*5\n",
    "\n",
    "            # Identity loss\n",
    "            Id_loss_B2A = criterion_Im(G_B2A(a_real), a_real)*10\n",
    "            Id_loss_A2B = criterion_Im(G_A2B(b_real), b_real)*10\n",
    "\n",
    "            # generator losses\n",
    "            Loss_G = Fool_disc_loss_A2B+Fool_disc_loss_B2A+Cycle_loss_A+Cycle_loss_B+Id_loss_B2A+Id_loss_A2B\n",
    "        \n",
    "\n",
    "            wandb.log({\n",
    "                \"total_g\": Loss_G,\n",
    "                \"total_d\": temp_D_losses\n",
    "            })\n",
    "\n",
    "            # Backward propagation\n",
    "            Loss_G.backward()\n",
    "            \n",
    "            # Optimisation step\n",
    "            optimizer_G_A2B.step()\n",
    "            optimizer_G_B2A.step()\n",
    "\n",
    "\n",
    "            if(iters == 0 and epoch == 0):\n",
    "                old_b_fake = b_fake.clone()\n",
    "                old_a_fake = a_fake.clone()\n",
    "            elif (old_b_fake.shape[0] == 5 and b_fake.shape[0]==1):\n",
    "                rand_int = random.randint(5, 24)\n",
    "                old_b_fake[rand_int-5:rand_int] = b_fake.clone()\n",
    "                old_a_fake[rand_int-5:rand_int] = a_fake.clone()\n",
    "            elif(old_b_fake.shape[0]< 25):\n",
    "                old_b_fake = torch.cat((b_fake.clone(),old_b_fake))\n",
    "                old_a_fake = torch.cat((a_fake.clone(),old_a_fake))\n",
    "\n",
    "            iters += 1\n",
    "            del a_real, b_real, a_fake, b_fake\n",
    "\n",
    "        print('[%d/%d]\\tFDL_A2B: %.4f\\tFDL_B2A: %.4f\\tCL_A: %.4f\\tCL_B: %.4f\\tID_B2A: %.4f\\tID_A2B: %.4f\\tLoss_D_A: %.4f\\tLoss_D_A: %.4f'\n",
    "                            % (epoch+1, epochs, Fool_disc_loss_A2B, Fool_disc_loss_B2A,Cycle_loss_A,Cycle_loss_B,Id_loss_B2A,Id_loss_A2B, Disc_loss_A.item(), Disc_loss_B.item()))\n",
    "        if (epoch%5==0):\n",
    "            save_models(G_A2B,G_B2A,D_A,D_B,str(epoch)+name)\n",
    "        iters = 0\n",
    "    \n",
    "    \n",
    "    return G_losses, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs, dataloader_mri, dataloader_sos, G_A2B, G_B2A, D_A, D_B, optimizer_G_A2B, optimizer_G_B2A, optimizer_D_A, optimizer_D_B,name):\n",
    "    \n",
    "    iters=0\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    print('Starting training loop')\n",
    "    # For each epoch\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # For each batch in the dataloader\n",
    "        for i,(data_mri, data_sos) in enumerate(zip(dataloader_mri, dataloader_sos),0):\n",
    "            temp_D_losses = 0\n",
    "            # print(b)\n",
    "            # Set model input\n",
    "            a_real = data_mri[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "            b_real = data_sos[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "            \n",
    "            # tensor_ones=torch.ones([a_real.shape[0],1,14,14]).to(device)\n",
    "            # tensor_zeros=torch.zeros([a_real.shape[0],1,14,14]).to(device)\n",
    "\n",
    "            # Generated images\n",
    "            # print(\"a_real shape: \"+str(a_real.shape))\n",
    "            b_fake = G_A2B(a_real)\n",
    "            # print(\"fake image generated\")\n",
    "            a_rec = G_B2A(b_fake)\n",
    "            a_fake = G_B2A(b_real)\n",
    "            b_rec = G_A2B(a_fake)\n",
    "\n",
    "            # CALCULATE DISCRIMINATORS LOSSES\n",
    "            # Discriminator A\n",
    "            optimizer_D_A.zero_grad()\n",
    "            if((iters > 0 or epoch > 0) and iters % 3 == 0):\n",
    "                rand_int = random.randint(1, old_a_fake.shape[0]-1)\n",
    "                Disc_loss_A = LSGAN_D(D_A(a_real), D_A(old_a_fake[rand_int-1:rand_int].detach()))\n",
    "                temp_D_losses += Disc_loss_A\n",
    "\n",
    "            else:\n",
    "                Disc_loss_A = LSGAN_D(D_A(a_real), D_A(a_fake.detach()))\n",
    "                temp_D_losses += Disc_loss_A\n",
    "\n",
    "            # Disc_loss_A = LSGAN_D(D_A(a_real), D_A(a_fake.detach()))\n",
    "            # D_A_losses.append(Disc_loss_A.item())\n",
    "            \n",
    "            Disc_loss_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            \n",
    "            # Discriminator B\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "            if((iters > 0 or epoch > 0) and iters % 3 == 0):\n",
    "                rand_int = random.randint(1, old_b_fake.shape[0]-1)\n",
    "                Disc_loss_B =  LSGAN_D(D_B(b_real), D_B(old_b_fake[rand_int-1:rand_int].detach()))\n",
    "                temp_D_losses += Disc_loss_B\n",
    "            else:\n",
    "                Disc_loss_B =  LSGAN_D(D_B(b_real), D_B(b_fake.detach()))\n",
    "                temp_D_losses += Disc_loss_B\n",
    "\n",
    "            Disc_loss_B.backward()\n",
    "            optimizer_D_B.step()   \n",
    "            \n",
    "            # different\n",
    "            # Generator\n",
    "            \n",
    "\n",
    "            optimizer_G_A2B.zero_grad()\n",
    "            optimizer_G_B2A.zero_grad()\n",
    "\n",
    "\n",
    "            # CALCULATE GENERATORS LOSSES\n",
    "            Fool_disc_loss_A2B = LSGAN_G(D_B(b_fake))\n",
    "            Fool_disc_loss_B2A = LSGAN_G(D_A(a_fake))\n",
    "\n",
    "            # Cycle Consistency    both use the two generators\n",
    "            Cycle_loss_A = criterion_Im(a_rec, a_real)*5\n",
    "            Cycle_loss_B = criterion_Im(b_rec, b_real)*5\n",
    "\n",
    "            # Identity loss\n",
    "            Id_loss_B2A = criterion_Im(G_B2A(a_real), a_real)*10\n",
    "            Id_loss_A2B = criterion_Im(G_A2B(b_real), b_real)*10\n",
    "\n",
    "            # generator losses\n",
    "            Loss_G = Fool_disc_loss_A2B+Fool_disc_loss_B2A+Cycle_loss_A+Cycle_loss_B+Id_loss_B2A+Id_loss_A2B\n",
    "        \n",
    "\n",
    "            wandb.log({\n",
    "                \"total_g\": Loss_G,\n",
    "                \"total_d\": temp_D_losses\n",
    "            })\n",
    "\n",
    "            # Backward propagation\n",
    "            Loss_G.backward()\n",
    "            \n",
    "            # Optimisation step\n",
    "            optimizer_G_A2B.step()\n",
    "            optimizer_G_B2A.step()\n",
    "\n",
    "\n",
    "            if(iters == 0 and epoch == 0):\n",
    "                old_b_fake = b_fake.clone()\n",
    "                old_a_fake = a_fake.clone()\n",
    "            elif (old_b_fake.shape[0] == 5 and b_fake.shape[0]==1):\n",
    "                rand_int = random.randint(5, 24)\n",
    "                old_b_fake[rand_int-5:rand_int] = b_fake.clone()\n",
    "                old_a_fake[rand_int-5:rand_int] = a_fake.clone()\n",
    "            elif(old_b_fake.shape[0]< 25):\n",
    "                old_b_fake = torch.cat((b_fake.clone(),old_b_fake))\n",
    "                old_a_fake = torch.cat((a_fake.clone(),old_a_fake))\n",
    "\n",
    "            iters += 1\n",
    "            del a_real, b_real, a_fake, b_fake\n",
    "\n",
    "        print('[%d/%d]\\tFDL_A2B: %.4f\\tFDL_B2A: %.4f\\tCL_A: %.4f\\tCL_B: %.4f\\tID_B2A: %.4f\\tID_A2B: %.4f\\tLoss_D_A: %.4f\\tLoss_D_A: %.4f'\n",
    "                            % (epoch+6, epochs+5, Fool_disc_loss_A2B, Fool_disc_loss_B2A,Cycle_loss_A,Cycle_loss_B,Id_loss_B2A,Id_loss_A2B, Disc_loss_A.item(), Disc_loss_B.item()))\n",
    "        save_models(G_A2B,G_B2A,D_A,D_B,str(epoch+5)+name)\n",
    "        iters = 0\n",
    "    \n",
    "    \n",
    "    return G_losses, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs, dataloader_mri, dataloader_sos, G_A2B, G_B2A, D_A, D_B, optimizer_G_A2B, optimizer_G_B2A, optimizer_D_A, optimizer_D_B,name):\n",
    "    fin = open(name+\"_gen.txt\",\"w\")\n",
    "    fdisc = open(name+\"_disc.txt\",\"w\")\n",
    "    iters=0\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    G_loss_epochs = []\n",
    "    print('Starting training loop')\n",
    "    # For each epoch\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # For each batch in the dataloader\n",
    "        for i,(data_mri, data_sos) in enumerate(zip(dataloader_mri, dataloader_sos),0):\n",
    "            temp_D_losses = 0\n",
    "            # print(b)\n",
    "            # Set model input\n",
    "            a_real = data_mri[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "            b_real = data_sos[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "            \n",
    "            # tensor_ones=torch.ones([a_real.shape[0],1,14,14]).to(device)\n",
    "            # tensor_zeros=torch.zeros([a_real.shape[0],1,14,14]).to(device)\n",
    "\n",
    "            # Generated images\n",
    "            # print(\"a_real shape: \"+str(a_real.shape))\n",
    "            b_fake = G_A2B(a_real)\n",
    "            # print(\"fake image generated\")\n",
    "            a_rec = G_B2A(b_fake)\n",
    "            a_fake = G_B2A(b_real)\n",
    "            b_rec = G_A2B(a_fake)\n",
    "\n",
    "            # CALCULATE DISCRIMINATORS LOSSES\n",
    "            # Discriminator A\n",
    "            optimizer_D_A.zero_grad()\n",
    "            if((iters > 0 or epoch > 0) and iters % 3 == 0):\n",
    "                rand_int = random.randint(1, old_a_fake.shape[0]-1)\n",
    "                Disc_loss_A = LSGAN_D(D_A(a_real), D_A(old_a_fake[rand_int-1:rand_int].detach()))\n",
    "                temp_D_losses += Disc_loss_A\n",
    "\n",
    "            else:\n",
    "                Disc_loss_A = LSGAN_D(D_A(a_real), D_A(a_fake.detach()))\n",
    "                temp_D_losses += Disc_loss_A\n",
    "\n",
    "            # Disc_loss_A = LSGAN_D(D_A(a_real), D_A(a_fake.detach()))\n",
    "            # D_A_losses.append(Disc_loss_A.item())\n",
    "            \n",
    "            Disc_loss_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            \n",
    "            # Discriminator B\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "            if((iters > 0 or epoch > 0) and iters % 3 == 0):\n",
    "                rand_int = random.randint(1, old_b_fake.shape[0]-1)\n",
    "                Disc_loss_B =  LSGAN_D(D_B(b_real), D_B(old_b_fake[rand_int-1:rand_int].detach()))\n",
    "                temp_D_losses += Disc_loss_B\n",
    "            else:\n",
    "                Disc_loss_B =  LSGAN_D(D_B(b_real), D_B(b_fake.detach()))\n",
    "                temp_D_losses += Disc_loss_B\n",
    "\n",
    "            Disc_loss_B.backward()\n",
    "            optimizer_D_B.step()   \n",
    "            \n",
    "            D_losses.append(temp_D_losses) # different\n",
    "            # Generator\n",
    "            \n",
    "\n",
    "            optimizer_G_A2B.zero_grad()\n",
    "            optimizer_G_B2A.zero_grad()\n",
    "\n",
    "\n",
    "            # CALCULATE GENERATORS LOSSES\n",
    "            Fool_disc_loss_A2B = LSGAN_G(D_B(b_fake))\n",
    "            Fool_disc_loss_B2A = LSGAN_G(D_A(a_fake))\n",
    "\n",
    "            # Cycle Consistency    both use the two generators\n",
    "            Cycle_loss_A = criterion_Im(a_rec, a_real)*5\n",
    "            Cycle_loss_B = criterion_Im(b_rec, b_real)*5\n",
    "\n",
    "            # Identity loss\n",
    "            Id_loss_B2A = criterion_Im(G_B2A(a_real), a_real)*10\n",
    "            Id_loss_A2B = criterion_Im(G_A2B(b_real), b_real)*10\n",
    "\n",
    "            # generator losses\n",
    "            Loss_G = Fool_disc_loss_A2B+Fool_disc_loss_B2A+Cycle_loss_A+Cycle_loss_B+Id_loss_B2A+Id_loss_A2B\n",
    "            G_losses.append(Loss_G)\n",
    "\n",
    "            wandb.log({\n",
    "                \"total_g\": Loss_G,\n",
    "                \"total_d\": temp_D_losses\n",
    "            })\n",
    "\n",
    "            # Backward propagation\n",
    "            Loss_G.backward()\n",
    "            \n",
    "            # Optimisation step\n",
    "            optimizer_G_A2B.step()\n",
    "            optimizer_G_B2A.step()\n",
    "\n",
    "\n",
    "            if(iters == 0 and epoch == 0):\n",
    "                old_b_fake = b_fake.clone()\n",
    "                old_a_fake = a_fake.clone()\n",
    "            elif (old_b_fake.shape[0] == 5 and b_fake.shape[0]==1):\n",
    "                rand_int = random.randint(5, 24)\n",
    "                old_b_fake[rand_int-5:rand_int] = b_fake.clone()\n",
    "                old_a_fake[rand_int-5:rand_int] = a_fake.clone()\n",
    "            elif(old_b_fake.shape[0]< 25):\n",
    "                old_b_fake = torch.cat((b_fake.clone(),old_b_fake))\n",
    "                old_a_fake = torch.cat((a_fake.clone(),old_a_fake))\n",
    "\n",
    "            iters += 1\n",
    "            del a_real, b_real, a_fake, b_fake\n",
    "\n",
    "\n",
    "            if iters % 50 == 0:\n",
    "            \n",
    "                print('[%d/%d]\\tFDL_A2B: %.4f\\tFDL_B2A: %.4f\\tCL_A: %.4f\\tCL_B: %.4f\\tID_B2A: %.4f\\tID_A2B: %.4f\\tLoss_D_A: %.4f\\tLoss_D_A: %.4f'\n",
    "                            % (epoch+1, epochs, Fool_disc_loss_A2B, Fool_disc_loss_B2A,Cycle_loss_A,Cycle_loss_B,Id_loss_B2A,\n",
    "                                Id_loss_A2B, Disc_loss_A.item(), Disc_loss_B.item()))\n",
    "                # print('[%d/%d]\\tFDL_A2B: %.4f\\tFDL_B2A: %.4f\\tCL_A: %.4f\\tCL_B: %.4f\\tLoss_D_A: %.4f\\tLoss_D_A: %.4f'\n",
    "                #             % (epoch+1, num_epochs, Fool_disc_loss_A2B, Fool_disc_loss_B2A,Cycle_loss_A,Cycle_loss_B, Disc_loss_A.item(), Disc_loss_B.item()))\n",
    "        save_models(G_A2B,G_B2A,D_A,D_B,str(epoch)+name)\n",
    "        iters = 0\n",
    "        G_loss_epochs.append(Loss_G)\n",
    "        D_loss_epochs.append(temp_D_losses)\n",
    "    fin.write(','.join(str(num) for num in G_loss_epochs))\n",
    "    fin.write(\"\\n\")\n",
    "    fdisc.write(','.join(str(num) for num in D_loss_epochs))\n",
    "    fin.write(\"\\n\")\n",
    "    return G_losses, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs, dataloader_mri, dataloader_sos, G_A2B, G_B2A, D_A, D_B, optimizer_G_A2B, optimizer_G_B2A, optimizer_D_A, optimizer_D_B,name):\n",
    "    \n",
    "    iters=0\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    \n",
    "    print('Starting training loop')\n",
    "    # For each epoch\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # For each batch in the dataloader\n",
    "        for i,(data_mri, data_sos) in enumerate(zip(dataloader_mri, dataloader_sos),0):\n",
    "            temp_D_losses = 0\n",
    "            # print(b)\n",
    "            # Set model input\n",
    "            a_real = data_mri[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "            b_real = data_sos[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "            \n",
    "            # tensor_ones=torch.ones([a_real.shape[0],1,14,14]).to(device)\n",
    "            # tensor_zeros=torch.zeros([a_real.shape[0],1,14,14]).to(device)\n",
    "\n",
    "            # Generated images\n",
    "            # print(\"a_real shape: \"+str(a_real.shape))\n",
    "            b_fake = G_A2B(a_real)\n",
    "            # print(\"fake image generated\")\n",
    "            a_rec = G_B2A(b_fake)\n",
    "            a_fake = G_B2A(b_real)\n",
    "            b_rec = G_A2B(a_fake)\n",
    "\n",
    "            # CALCULATE DISCRIMINATORS LOSSES\n",
    "            # Discriminator A\n",
    "            optimizer_D_A.zero_grad()\n",
    "            if((iters > 0 or epoch > 0) and iters % 3 == 0):\n",
    "                rand_int = random.randint(1, old_a_fake.shape[0]-1)\n",
    "                Disc_loss_A = LSGAN_D(D_A(a_real), D_A(old_a_fake[rand_int-1:rand_int].detach()))\n",
    "                temp_D_losses += Disc_loss_A\n",
    "\n",
    "            else:\n",
    "                Disc_loss_A = LSGAN_D(D_A(a_real), D_A(a_fake.detach()))\n",
    "                temp_D_losses += Disc_loss_A\n",
    "\n",
    "            # Disc_loss_A = LSGAN_D(D_A(a_real), D_A(a_fake.detach()))\n",
    "            # D_A_losses.append(Disc_loss_A.item())\n",
    "            \n",
    "            Disc_loss_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            \n",
    "            # Discriminator B\n",
    "\n",
    "            optimizer_D_B.zero_grad()\n",
    "            if((iters > 0 or epoch > 0) and iters % 3 == 0):\n",
    "                rand_int = random.randint(1, old_b_fake.shape[0]-1)\n",
    "                Disc_loss_B =  LSGAN_D(D_B(b_real), D_B(old_b_fake[rand_int-1:rand_int].detach()))\n",
    "                temp_D_losses += Disc_loss_B\n",
    "            else:\n",
    "                Disc_loss_B =  LSGAN_D(D_B(b_real), D_B(b_fake.detach()))\n",
    "                temp_D_losses += Disc_loss_B\n",
    "\n",
    "            Disc_loss_B.backward()\n",
    "            optimizer_D_B.step()   \n",
    "            \n",
    "            D_losses.append(temp_D_losses) # different\n",
    "            # Generator\n",
    "            \n",
    "\n",
    "            optimizer_G_A2B.zero_grad()\n",
    "            optimizer_G_B2A.zero_grad()\n",
    "\n",
    "\n",
    "            # CALCULATE GENERATORS LOSSES\n",
    "            Fool_disc_loss_A2B = LSGAN_G(D_B(b_fake))\n",
    "            Fool_disc_loss_B2A = LSGAN_G(D_A(a_fake))\n",
    "\n",
    "            # Cycle Consistency    both use the two generators\n",
    "            Cycle_loss_A = criterion_Im(a_rec, a_real)*5\n",
    "            Cycle_loss_B = criterion_Im(b_rec, b_real)*5\n",
    "\n",
    "            # Identity loss\n",
    "            Id_loss_B2A = criterion_Im(G_B2A(a_real), a_real)*10\n",
    "            Id_loss_A2B = criterion_Im(G_A2B(b_real), b_real)*10\n",
    "\n",
    "            # generator losses\n",
    "            Loss_G = Fool_disc_loss_A2B+Fool_disc_loss_B2A+Cycle_loss_A+Cycle_loss_B+Id_loss_B2A+Id_loss_A2B\n",
    "            G_losses.append(Loss_G)\n",
    "\n",
    "            wandb.log({\n",
    "                \"total_g\": Loss_G,\n",
    "                \"total_d\": temp_D_losses\n",
    "            })\n",
    "\n",
    "            # Backward propagation\n",
    "            Loss_G.backward()\n",
    "            \n",
    "            # Optimisation step\n",
    "            optimizer_G_A2B.step()\n",
    "            optimizer_G_B2A.step()\n",
    "\n",
    "\n",
    "            if(iters == 0 and epoch == 0):\n",
    "                old_b_fake = b_fake.clone()\n",
    "                old_a_fake = a_fake.clone()\n",
    "            elif (old_b_fake.shape[0] == 5 and b_fake.shape[0]==1):\n",
    "                rand_int = random.randint(5, 24)\n",
    "                old_b_fake[rand_int-5:rand_int] = b_fake.clone()\n",
    "                old_a_fake[rand_int-5:rand_int] = a_fake.clone()\n",
    "            elif(old_b_fake.shape[0]< 25):\n",
    "                old_b_fake = torch.cat((b_fake.clone(),old_b_fake))\n",
    "                old_a_fake = torch.cat((a_fake.clone(),old_a_fake))\n",
    "\n",
    "            iters += 1\n",
    "            del a_real, b_real, a_fake, b_fake\n",
    "\n",
    "\n",
    "            if iters % 50 == 0:\n",
    "            \n",
    "                print('[%d/%d]\\tFDL_A2B: %.4f\\tFDL_B2A: %.4f\\tCL_A: %.4f\\tCL_B: %.4f\\tID_B2A: %.4f\\tID_A2B: %.4f\\tLoss_D_A: %.4f\\tLoss_D_A: %.4f'\n",
    "                            % (epoch+1, epochs, Fool_disc_loss_A2B, Fool_disc_loss_B2A,Cycle_loss_A,Cycle_loss_B,Id_loss_B2A,\n",
    "                                Id_loss_A2B, Disc_loss_A.item(), Disc_loss_B.item()))\n",
    "                # print('[%d/%d]\\tFDL_A2B: %.4f\\tFDL_B2A: %.4f\\tCL_A: %.4f\\tCL_B: %.4f\\tLoss_D_A: %.4f\\tLoss_D_A: %.4f'\n",
    "                #             % (epoch+1, num_epochs, Fool_disc_loss_A2B, Fool_disc_loss_B2A,Cycle_loss_A,Cycle_loss_B, Disc_loss_A.item(), Disc_loss_B.item()))\n",
    "        save_models(G_A2B,G_B2A,D_A,D_B,\"test_\"+name+str(epoch))\n",
    "        iters = 0            \n",
    "    return G_losses, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Xiaowei/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2l70q1h3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d88d2f518144579ebaf112bb44245d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>total_d</td><td>▄▇▃▇▆▇▃▇▃▂▆▇▄▄▄▂▇█▇▁▅▃▄▇▆▃▇▃▂▅▇▃▁▇▂██▅▃▇</td></tr><tr><td>total_g</td><td>█▅▆▆▆▂▄▃▅▅▃▃▅▅▄▂▁▃▃▃▄▁▄▂▂▂▂▁▂▃▂▂▄▂▂▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_d</td><td>1.06026</td></tr><tr><td>total_g</td><td>0.75421</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bs4_dl4_lr210-4_b09</strong> at: <a href=\"https://wandb.ai/powerr/mri2sos/runs/2l70q1h3\" target=\"_blank\">https://wandb.ai/powerr/mri2sos/runs/2l70q1h3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230411_161820-2l70q1h3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2l70q1h3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d7b20700cd4059908a3abefa2ef209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Xiaowei\\Desktop\\Clara\\CycleGAN\\wandb\\run-20230412_003257-6ymiwj7p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/powerr/mri2sos/runs/6ymiwj7p\" target=\"_blank\">skull_bs4_dl4_lr210-4_b09</a></strong> to <a href=\"https://wandb.ai/powerr/mri2sos\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/powerr/mri2sos\" target=\"_blank\">https://wandb.ai/powerr/mri2sos</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/powerr/mri2sos/runs/6ymiwj7p\" target=\"_blank\">https://wandb.ai/powerr/mri2sos/runs/6ymiwj7p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop\n",
      "[1/100]\tFDL_A2B: 0.2181\tFDL_B2A: 0.2437\tCL_A: 0.1306\tCL_B: 0.1411\tID_B2A: 0.2396\tID_A2B: 0.2656\tLoss_D_A: 0.4528\tLoss_D_A: 0.4781\n",
      "[2/100]\tFDL_A2B: 0.3692\tFDL_B2A: 0.2677\tCL_A: 0.1627\tCL_B: 0.1490\tID_B2A: 0.3329\tID_A2B: 0.1914\tLoss_D_A: 0.4369\tLoss_D_A: 0.4538\n",
      "[3/100]\tFDL_A2B: 0.1985\tFDL_B2A: 0.1263\tCL_A: 0.0579\tCL_B: 0.0860\tID_B2A: 0.0993\tID_A2B: 0.1288\tLoss_D_A: 0.6266\tLoss_D_A: 0.5603\n",
      "[4/100]\tFDL_A2B: 0.1556\tFDL_B2A: 0.1649\tCL_A: 0.0554\tCL_B: 0.1264\tID_B2A: 0.0927\tID_A2B: 0.2195\tLoss_D_A: 0.5891\tLoss_D_A: 0.5470\n",
      "[5/100]\tFDL_A2B: 0.1742\tFDL_B2A: 0.2018\tCL_A: 0.1273\tCL_B: 0.0576\tID_B2A: 0.2245\tID_A2B: 0.0996\tLoss_D_A: 0.4313\tLoss_D_A: 0.5135\n",
      "[6/100]\tFDL_A2B: 0.1512\tFDL_B2A: 0.1599\tCL_A: 0.0712\tCL_B: 0.2828\tID_B2A: 0.0791\tID_A2B: 0.5630\tLoss_D_A: 0.5198\tLoss_D_A: 0.6503\n",
      "[7/100]\tFDL_A2B: 0.1150\tFDL_B2A: 0.1185\tCL_A: 0.0541\tCL_B: 0.0539\tID_B2A: 0.0716\tID_A2B: 0.0696\tLoss_D_A: 0.7431\tLoss_D_A: 0.7329\n",
      "[8/100]\tFDL_A2B: 0.1533\tFDL_B2A: 0.2169\tCL_A: 0.1398\tCL_B: 0.0387\tID_B2A: 0.1851\tID_A2B: 0.0933\tLoss_D_A: 0.4514\tLoss_D_A: 0.5181\n",
      "[9/100]\tFDL_A2B: 0.1356\tFDL_B2A: 0.1292\tCL_A: 0.0714\tCL_B: 0.0475\tID_B2A: 0.0597\tID_A2B: 0.0662\tLoss_D_A: 0.6497\tLoss_D_A: 0.7474\n",
      "[10/100]\tFDL_A2B: 0.1050\tFDL_B2A: 0.1761\tCL_A: 0.2048\tCL_B: 0.1176\tID_B2A: 0.3927\tID_A2B: 0.1796\tLoss_D_A: 0.5082\tLoss_D_A: 0.6335\n",
      "[11/100]\tFDL_A2B: 0.2433\tFDL_B2A: 0.3084\tCL_A: 0.0607\tCL_B: 0.0563\tID_B2A: 0.1101\tID_A2B: 0.0463\tLoss_D_A: 0.3381\tLoss_D_A: 0.2870\n",
      "[12/100]\tFDL_A2B: 0.1362\tFDL_B2A: 0.1505\tCL_A: 0.0739\tCL_B: 0.0877\tID_B2A: 0.1080\tID_A2B: 0.1469\tLoss_D_A: 0.5598\tLoss_D_A: 0.5734\n",
      "[13/100]\tFDL_A2B: 0.1832\tFDL_B2A: 0.1727\tCL_A: 0.0368\tCL_B: 0.0329\tID_B2A: 0.0678\tID_A2B: 0.0547\tLoss_D_A: 0.5056\tLoss_D_A: 0.5457\n",
      "[14/100]\tFDL_A2B: 0.2213\tFDL_B2A: 0.1589\tCL_A: 0.0572\tCL_B: 0.0434\tID_B2A: 0.0550\tID_A2B: 0.0816\tLoss_D_A: 0.6321\tLoss_D_A: 0.4233\n",
      "[15/100]\tFDL_A2B: 0.3057\tFDL_B2A: 0.2083\tCL_A: 0.0230\tCL_B: 0.0208\tID_B2A: 0.0428\tID_A2B: 0.0413\tLoss_D_A: 0.4541\tLoss_D_A: 0.3761\n",
      "[16/100]\tFDL_A2B: 0.3030\tFDL_B2A: 0.2571\tCL_A: 0.0318\tCL_B: 0.2208\tID_B2A: 0.0521\tID_A2B: 0.5101\tLoss_D_A: 0.4285\tLoss_D_A: 0.5559\n",
      "[17/100]\tFDL_A2B: 0.1121\tFDL_B2A: 0.2072\tCL_A: 0.0289\tCL_B: 0.1135\tID_B2A: 0.0527\tID_A2B: 0.1758\tLoss_D_A: 0.4810\tLoss_D_A: 0.5718\n",
      "[18/100]\tFDL_A2B: 0.1455\tFDL_B2A: 0.2577\tCL_A: 0.0382\tCL_B: 0.0274\tID_B2A: 0.0985\tID_A2B: 0.0559\tLoss_D_A: 0.4805\tLoss_D_A: 0.5795\n",
      "[19/100]\tFDL_A2B: 0.1097\tFDL_B2A: 0.1010\tCL_A: 0.0458\tCL_B: 0.0466\tID_B2A: 0.0681\tID_A2B: 0.0651\tLoss_D_A: 0.6934\tLoss_D_A: 0.7053\n",
      "[20/100]\tFDL_A2B: 0.2042\tFDL_B2A: 0.1525\tCL_A: 0.0206\tCL_B: 0.0366\tID_B2A: 0.0311\tID_A2B: 0.1071\tLoss_D_A: 0.5361\tLoss_D_A: 0.4970\n",
      "[21/100]\tFDL_A2B: 0.1518\tFDL_B2A: 0.1607\tCL_A: 0.0321\tCL_B: 0.0726\tID_B2A: 0.0386\tID_A2B: 0.0917\tLoss_D_A: 0.5247\tLoss_D_A: 0.5686\n",
      "[22/100]\tFDL_A2B: 0.2945\tFDL_B2A: 0.3164\tCL_A: 0.0356\tCL_B: 0.0692\tID_B2A: 0.0529\tID_A2B: 0.0856\tLoss_D_A: 0.4698\tLoss_D_A: 0.2145\n",
      "[23/100]\tFDL_A2B: 0.3143\tFDL_B2A: 0.1883\tCL_A: 0.0205\tCL_B: 0.0526\tID_B2A: 0.0310\tID_A2B: 0.0522\tLoss_D_A: 0.5325\tLoss_D_A: 0.2991\n",
      "[24/100]\tFDL_A2B: 0.1073\tFDL_B2A: 0.1755\tCL_A: 0.0451\tCL_B: 0.0834\tID_B2A: 0.0615\tID_A2B: 0.0509\tLoss_D_A: 0.6231\tLoss_D_A: 0.4816\n",
      "[25/100]\tFDL_A2B: 0.1559\tFDL_B2A: 0.1646\tCL_A: 0.0237\tCL_B: 0.0192\tID_B2A: 0.0541\tID_A2B: 0.0289\tLoss_D_A: 0.5541\tLoss_D_A: 0.5579\n",
      "[26/100]\tFDL_A2B: 0.2001\tFDL_B2A: 0.1267\tCL_A: 0.0764\tCL_B: 0.0309\tID_B2A: 0.1170\tID_A2B: 0.0454\tLoss_D_A: 0.5621\tLoss_D_A: 0.5521\n",
      "[27/100]\tFDL_A2B: 0.1200\tFDL_B2A: 0.2393\tCL_A: 0.0636\tCL_B: 0.0292\tID_B2A: 0.0544\tID_A2B: 0.0484\tLoss_D_A: 0.4974\tLoss_D_A: 0.5602\n",
      "[28/100]\tFDL_A2B: 0.3024\tFDL_B2A: 0.1909\tCL_A: 0.0272\tCL_B: 0.0927\tID_B2A: 0.0349\tID_A2B: 0.1603\tLoss_D_A: 0.5327\tLoss_D_A: 0.3373\n",
      "[29/100]\tFDL_A2B: 0.1608\tFDL_B2A: 0.2295\tCL_A: 0.0431\tCL_B: 0.0279\tID_B2A: 0.0575\tID_A2B: 0.0428\tLoss_D_A: 0.4594\tLoss_D_A: 0.5160\n",
      "[30/100]\tFDL_A2B: 0.2026\tFDL_B2A: 0.1058\tCL_A: 0.1904\tCL_B: 0.0183\tID_B2A: 0.3584\tID_A2B: 0.0426\tLoss_D_A: 0.6243\tLoss_D_A: 0.5280\n",
      "[31/100]\tFDL_A2B: 0.1970\tFDL_B2A: 0.1659\tCL_A: 0.0286\tCL_B: 0.0385\tID_B2A: 0.0417\tID_A2B: 0.0692\tLoss_D_A: 0.5248\tLoss_D_A: 0.5064\n",
      "[32/100]\tFDL_A2B: 0.2231\tFDL_B2A: 0.1112\tCL_A: 0.0323\tCL_B: 0.0205\tID_B2A: 0.0602\tID_A2B: 0.0233\tLoss_D_A: 0.5681\tLoss_D_A: 0.5185\n",
      "[33/100]\tFDL_A2B: 0.1821\tFDL_B2A: 0.2006\tCL_A: 0.0368\tCL_B: 0.0558\tID_B2A: 0.0487\tID_A2B: 0.0620\tLoss_D_A: 0.4808\tLoss_D_A: 0.4768\n",
      "[34/100]\tFDL_A2B: 0.1576\tFDL_B2A: 0.1591\tCL_A: 0.0198\tCL_B: 0.0343\tID_B2A: 0.0262\tID_A2B: 0.0606\tLoss_D_A: 0.5517\tLoss_D_A: 0.4976\n",
      "[35/100]\tFDL_A2B: 0.1729\tFDL_B2A: 0.2325\tCL_A: 0.0264\tCL_B: 0.0363\tID_B2A: 0.0455\tID_A2B: 0.0401\tLoss_D_A: 0.4224\tLoss_D_A: 0.5076\n",
      "[36/100]\tFDL_A2B: 0.2200\tFDL_B2A: 0.1378\tCL_A: 0.0323\tCL_B: 0.1267\tID_B2A: 0.0778\tID_A2B: 0.2547\tLoss_D_A: 0.5561\tLoss_D_A: 0.5084\n",
      "[37/100]\tFDL_A2B: 0.3823\tFDL_B2A: 0.1643\tCL_A: 0.0209\tCL_B: 0.0170\tID_B2A: 0.0335\tID_A2B: 0.0229\tLoss_D_A: 0.4432\tLoss_D_A: 0.3289\n",
      "[38/100]\tFDL_A2B: 0.1942\tFDL_B2A: 0.1653\tCL_A: 0.0258\tCL_B: 0.0370\tID_B2A: 0.0669\tID_A2B: 0.0686\tLoss_D_A: 0.4929\tLoss_D_A: 0.5019\n",
      "[39/100]\tFDL_A2B: 0.2132\tFDL_B2A: 0.1475\tCL_A: 0.0317\tCL_B: 0.0278\tID_B2A: 0.0494\tID_A2B: 0.0272\tLoss_D_A: 0.5455\tLoss_D_A: 0.5053\n",
      "[40/100]\tFDL_A2B: 0.2729\tFDL_B2A: 0.2869\tCL_A: 0.0300\tCL_B: 0.0897\tID_B2A: 0.0464\tID_A2B: 0.0344\tLoss_D_A: 0.3930\tLoss_D_A: 0.2586\n",
      "[41/100]\tFDL_A2B: 0.1861\tFDL_B2A: 0.1770\tCL_A: 0.0235\tCL_B: 0.0411\tID_B2A: 0.0267\tID_A2B: 0.0605\tLoss_D_A: 0.4946\tLoss_D_A: 0.4837\n",
      "[42/100]\tFDL_A2B: 0.1768\tFDL_B2A: 0.1397\tCL_A: 0.0274\tCL_B: 0.0205\tID_B2A: 0.0343\tID_A2B: 0.0421\tLoss_D_A: 0.6792\tLoss_D_A: 0.5031\n",
      "[43/100]\tFDL_A2B: 0.4555\tFDL_B2A: 0.2805\tCL_A: 0.0168\tCL_B: 0.0399\tID_B2A: 0.0264\tID_A2B: 0.0477\tLoss_D_A: 0.2727\tLoss_D_A: 0.3411\n",
      "[44/100]\tFDL_A2B: 0.2117\tFDL_B2A: 0.2216\tCL_A: 0.0240\tCL_B: 0.0208\tID_B2A: 0.0351\tID_A2B: 0.0380\tLoss_D_A: 0.4485\tLoss_D_A: 0.4617\n",
      "[45/100]\tFDL_A2B: 0.0894\tFDL_B2A: 0.1388\tCL_A: 0.0652\tCL_B: 0.0137\tID_B2A: 0.0299\tID_A2B: 0.0306\tLoss_D_A: 0.7618\tLoss_D_A: 0.7616\n",
      "[46/100]\tFDL_A2B: 0.1571\tFDL_B2A: 0.2193\tCL_A: 0.0247\tCL_B: 0.0390\tID_B2A: 0.0353\tID_A2B: 0.0537\tLoss_D_A: 0.4972\tLoss_D_A: 0.5134\n",
      "[47/100]\tFDL_A2B: 0.1873\tFDL_B2A: 0.2059\tCL_A: 0.0307\tCL_B: 0.0592\tID_B2A: 0.0627\tID_A2B: 0.0722\tLoss_D_A: 0.5186\tLoss_D_A: 0.4991\n",
      "[48/100]\tFDL_A2B: 0.1735\tFDL_B2A: 0.2226\tCL_A: 0.0338\tCL_B: 0.0202\tID_B2A: 0.0624\tID_A2B: 0.0295\tLoss_D_A: 0.3852\tLoss_D_A: 0.4760\n",
      "[49/100]\tFDL_A2B: 0.3203\tFDL_B2A: 0.1854\tCL_A: 0.0249\tCL_B: 0.0182\tID_B2A: 0.0282\tID_A2B: 0.0461\tLoss_D_A: 0.5235\tLoss_D_A: 0.3579\n",
      "[50/100]\tFDL_A2B: 0.1375\tFDL_B2A: 0.1792\tCL_A: 0.0429\tCL_B: 0.0316\tID_B2A: 0.0607\tID_A2B: 0.0367\tLoss_D_A: 0.5249\tLoss_D_A: 0.7131\n",
      "[51/100]\tFDL_A2B: 0.1112\tFDL_B2A: 0.3014\tCL_A: 0.0251\tCL_B: 0.0208\tID_B2A: 0.0224\tID_A2B: 0.0363\tLoss_D_A: 0.3881\tLoss_D_A: 0.5768\n",
      "[52/100]\tFDL_A2B: 0.1468\tFDL_B2A: 0.1715\tCL_A: 0.0188\tCL_B: 0.0444\tID_B2A: 0.0305\tID_A2B: 0.0749\tLoss_D_A: 0.5135\tLoss_D_A: 0.5460\n",
      "[53/100]\tFDL_A2B: 0.1745\tFDL_B2A: 0.0939\tCL_A: 0.0196\tCL_B: 0.0252\tID_B2A: 0.0355\tID_A2B: 0.2319\tLoss_D_A: 0.7770\tLoss_D_A: 0.7626\n",
      "[54/100]\tFDL_A2B: 0.1593\tFDL_B2A: 0.1624\tCL_A: 0.0264\tCL_B: 0.0254\tID_B2A: 0.0288\tID_A2B: 0.0240\tLoss_D_A: 0.5472\tLoss_D_A: 0.5147\n",
      "[55/100]\tFDL_A2B: 0.1224\tFDL_B2A: 0.1849\tCL_A: 0.0298\tCL_B: 0.0128\tID_B2A: 0.0362\tID_A2B: 0.0170\tLoss_D_A: 0.5231\tLoss_D_A: 0.5776\n",
      "[56/100]\tFDL_A2B: 0.3591\tFDL_B2A: 0.2631\tCL_A: 0.0280\tCL_B: 0.0198\tID_B2A: 0.0268\tID_A2B: 0.0324\tLoss_D_A: 0.4104\tLoss_D_A: 0.3904\n",
      "[57/100]\tFDL_A2B: 0.1547\tFDL_B2A: 0.1583\tCL_A: 0.0186\tCL_B: 0.0164\tID_B2A: 0.0223\tID_A2B: 0.0279\tLoss_D_A: 0.5322\tLoss_D_A: 0.5587\n",
      "[58/100]\tFDL_A2B: 0.1614\tFDL_B2A: 0.1564\tCL_A: 0.0191\tCL_B: 0.0217\tID_B2A: 0.0245\tID_A2B: 0.0424\tLoss_D_A: 0.5210\tLoss_D_A: 0.5299\n",
      "[59/100]\tFDL_A2B: 0.1444\tFDL_B2A: 0.1522\tCL_A: 0.0169\tCL_B: 0.0192\tID_B2A: 0.0354\tID_A2B: 0.0364\tLoss_D_A: 0.5688\tLoss_D_A: 0.5116\n",
      "[60/100]\tFDL_A2B: 0.1568\tFDL_B2A: 0.1938\tCL_A: 0.0416\tCL_B: 0.0384\tID_B2A: 0.0481\tID_A2B: 0.0515\tLoss_D_A: 0.4990\tLoss_D_A: 0.5380\n",
      "[61/100]\tFDL_A2B: 0.3375\tFDL_B2A: 0.1260\tCL_A: 0.0194\tCL_B: 0.0310\tID_B2A: 0.0319\tID_A2B: 0.0476\tLoss_D_A: 0.5333\tLoss_D_A: 0.3666\n",
      "[62/100]\tFDL_A2B: 0.3276\tFDL_B2A: 0.2706\tCL_A: 0.0245\tCL_B: 0.0140\tID_B2A: 0.0336\tID_A2B: 0.0194\tLoss_D_A: 0.4082\tLoss_D_A: 0.3424\n",
      "[63/100]\tFDL_A2B: 0.1382\tFDL_B2A: 0.1787\tCL_A: 0.0160\tCL_B: 0.0187\tID_B2A: 0.0286\tID_A2B: 0.0267\tLoss_D_A: 0.4673\tLoss_D_A: 0.5901\n",
      "[64/100]\tFDL_A2B: 0.1588\tFDL_B2A: 0.1924\tCL_A: 0.0172\tCL_B: 0.0224\tID_B2A: 0.0354\tID_A2B: 0.0426\tLoss_D_A: 0.4957\tLoss_D_A: 0.4918\n",
      "[65/100]\tFDL_A2B: 0.2572\tFDL_B2A: 0.1160\tCL_A: 0.0243\tCL_B: 0.0336\tID_B2A: 0.0433\tID_A2B: 0.0349\tLoss_D_A: 0.5552\tLoss_D_A: 0.5097\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login 2511293b7c38a80081cadf8f2737b2f6fa992e70\n",
    "#wandb.init(project=\"mri2sos\", entity=\"powerr\")\n",
    "#name = [\"resnet0\",\"resnet1\",\"resnet2\",\"resnet3\",\"resnet4\",\"resnet5\",\"resnet6\",\"resnet7\",\"resnet8\",\"resnet9\",\"disc1\",\"disc3\",\"disc0\",\"gener0\",\"gener1\",\"gener3\",\"gener4\"]\n",
    "#name = [\"resnet83\",\"resnet93\"]\n",
    "name = [\"skull_bs4_dl4_lr210-4_b09\"]\n",
    "#name = [\"a1_2\",\"a2_2\",\"a3_2\",\"a4_2\",\"a5_2\",\"a6_2\",\"a7_2\",\"a8_2\",\"a9_2\"]\n",
    "#name = [\"a7\",\"a8\",\"a9\"]\n",
    "# name = [\"lr0001\",\"gendown3\",\"bs4\"]\n",
    "#name = [\"bs4lr0001b06\",\"bs4lr0001b09\",\"bs4lr0001b06dis4\",\"bs4lr0002b09\"]\n",
    "ps = []\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=0, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=1, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=2, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=3, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=4, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=8, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=1, gen_n_blocks=9, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=5, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=5, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=6, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=7, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=1, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=3, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=0, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=0, gen_n_blocks=9, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=3, gen_n_blocks=9, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=4, gen_n_blocks=9, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=4, lr=0.0001, beta1=0.6,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=4, lr=0.0001, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=4, lr=0.0002, beta1=0.6,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=4, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=0, dis_n_down=4, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=6, dis_n_down=4, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=1, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=18, dis_n_down=4, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=3, gen_n_blocks=9, dis_n_down=4, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=2, lr=0.0001, beta1=0.6,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=2, lr=0.0001, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=4, lr=0.0001, beta1=0.6,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=1, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=2, lr=0.0001, beta1=0.9,beta1_disc=0.9))\n",
    "# ps.append(Parameters(bs=1, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=3, gen_n_blocks=9, dis_n_down=2, lr=0.0001, beta1=0.9,beta1_disc=0.6))\n",
    "\n",
    "# ps.append(Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=4, lr=0.0001, beta1=0.9,beta1_disc=0.9))\n",
    "#ps.append(Parameters(bs=1, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=1, gen_n_blocks=6, dis_n_down=2, lr=0.001, beta1=0.6))\n",
    "# ps.append(Parameters(bs=1, n_channels=1, ngf=128, ndf=64, size=64, gen_n_down=1, gen_n_blocks=8, dis_n_down=1, lr=0.001, beta1=0.6))\n",
    "# ps.append(Parameters(bs=1, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=9, dis_n_down=2, lr=0.001, beta1=0.9))\n",
    "# ps.append(Parameters(bs=1, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=3, gen_n_blocks=6, dis_n_down=2, lr=0.002, beta1=0.6))\n",
    "# ps.append(Parameters(bs=1, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=4, gen_n_blocks=6, dis_n_down=4, lr=0.001, beta1=0.6))\n",
    "# ps.append(Parameters(bs=1, n_channels=1, ngf=64, ndf=16, size=64, gen_n_down=4, gen_n_blocks=1, dis_n_down=4, lr=0.001, beta1=0.6))\n",
    "# ps.append(Parameters(bs=1, n_channels=1, ngf=32, ndf=64, size=64, gen_n_down=1, gen_n_blocks=6, dis_n_down=2, lr=0.001, beta1=0.6))\n",
    "\n",
    "#for j in range(5):\n",
    "for i in range(len(ps)):\n",
    "        p = ps[i]\n",
    "        wandb.init(project=\"mri2sos\", entity=\"powerr\", name=name[i],config = {\n",
    "             \"ngf\" : p.ngf,\n",
    "             \"ndf\" : p.ndf,\n",
    "             \"gen_n_down\" : p.gen_n_down,\n",
    "             \"gen_n_blocks\" : p.gen_n_blocks,\n",
    "             \"dis_n_down\" : p.dis_n_down,\n",
    "             \"lr\" : p.lr,\n",
    "             \"beta1\" : p.beta1,\n",
    "             \"beta1_disc\" : p.beta1_disc\n",
    "\n",
    "             })\n",
    "\n",
    "\n",
    "        sweep(p,name[i])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:eg974la5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>total_d</td><td>▄▃▆▃▃▂▃▄▂▅▅▂▁▄▂▂▇▅▄▁▂▃▅▅▃▃▄▂▂▃▇▆▅▆▃▂▃█▃▇</td></tr><tr><td>total_g</td><td>▆▆▄▆▆█▅▅█▄▄▃▄▆▆▄▄▅▃▄▃▅▁▂▆▂▅▃▂▆▂▂▃▃▄▃▄▂▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_d</td><td>1.12719</td></tr><tr><td>total_g</td><td>2.03593</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bs1res9</strong> at: <a href=\"https://wandb.ai/powerr/mri2sos/runs/eg974la5\" target=\"_blank\">https://wandb.ai/powerr/mri2sos/runs/eg974la5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230306_200345-eg974la5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:eg974la5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be3dbe650f74c5789dcad3b13cf28c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Xiaowei\\Desktop\\Clara\\CycleGAN\\wandb\\run-20230306_202642-yqcdbjg7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/powerr/mri2sos/runs/yqcdbjg7\" target=\"_blank\">5res0</a></strong> to <a href=\"https://wandb.ai/powerr/mri2sos\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/powerr/mri2sos\" target=\"_blank\">https://wandb.ai/powerr/mri2sos</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/powerr/mri2sos/runs/yqcdbjg7\" target=\"_blank\">https://wandb.ai/powerr/mri2sos/runs/yqcdbjg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop\n",
      "[1/5]\tFDL_A2B: 0.2633\tFDL_B2A: 0.2118\tCL_A: 0.3105\tCL_B: 0.2901\tID_B2A: 0.5428\tID_A2B: 0.5094\tLoss_D_A: 0.6140\tLoss_D_A: 0.5006\n",
      "[1/5]\tFDL_A2B: 0.3229\tFDL_B2A: 0.2510\tCL_A: 0.2484\tCL_B: 0.1674\tID_B2A: 0.4442\tID_A2B: 0.3174\tLoss_D_A: 0.1742\tLoss_D_A: 0.1792\n",
      "[1/5]\tFDL_A2B: 0.1625\tFDL_B2A: 0.3132\tCL_A: 0.2824\tCL_B: 0.1952\tID_B2A: 0.4668\tID_A2B: 0.2771\tLoss_D_A: 0.4442\tLoss_D_A: 0.5533\n",
      "[1/5]\tFDL_A2B: 0.2958\tFDL_B2A: 0.6694\tCL_A: 0.2812\tCL_B: 0.2106\tID_B2A: 0.4639\tID_A2B: 0.3854\tLoss_D_A: 0.0904\tLoss_D_A: 0.3965\n",
      "[1/5]\tFDL_A2B: 0.1857\tFDL_B2A: 0.7567\tCL_A: 0.2241\tCL_B: 0.1702\tID_B2A: 0.3314\tID_A2B: 0.3222\tLoss_D_A: 0.1384\tLoss_D_A: 0.4520\n",
      "[1/5]\tFDL_A2B: 0.2323\tFDL_B2A: 0.9933\tCL_A: 0.2395\tCL_B: 0.1527\tID_B2A: 0.3969\tID_A2B: 0.2660\tLoss_D_A: 0.2344\tLoss_D_A: 0.4274\n",
      "[1/5]\tFDL_A2B: 0.1921\tFDL_B2A: 0.6094\tCL_A: 0.2167\tCL_B: 0.1544\tID_B2A: 0.3648\tID_A2B: 0.3136\tLoss_D_A: 0.2976\tLoss_D_A: 0.4910\n",
      "[1/5]\tFDL_A2B: 0.2670\tFDL_B2A: 0.6736\tCL_A: 0.1727\tCL_B: 0.2186\tID_B2A: 0.2705\tID_A2B: 0.4188\tLoss_D_A: 0.0649\tLoss_D_A: 0.3150\n",
      "[2/5]\tFDL_A2B: 0.2685\tFDL_B2A: 0.3727\tCL_A: 0.2082\tCL_B: 0.1379\tID_B2A: 0.3222\tID_A2B: 0.2678\tLoss_D_A: 0.3745\tLoss_D_A: 0.3612\n",
      "[2/5]\tFDL_A2B: 0.2540\tFDL_B2A: 0.8973\tCL_A: 0.2202\tCL_B: 0.1455\tID_B2A: 0.3305\tID_A2B: 0.3028\tLoss_D_A: 0.0489\tLoss_D_A: 0.1693\n",
      "[2/5]\tFDL_A2B: 0.1657\tFDL_B2A: 0.9604\tCL_A: 0.2182\tCL_B: 0.1944\tID_B2A: 0.3582\tID_A2B: 0.4038\tLoss_D_A: 0.0205\tLoss_D_A: 0.5335\n",
      "[2/5]\tFDL_A2B: 0.3500\tFDL_B2A: 0.9454\tCL_A: 0.2101\tCL_B: 0.1346\tID_B2A: 0.3443\tID_A2B: 0.2063\tLoss_D_A: 0.0555\tLoss_D_A: 0.3016\n",
      "[2/5]\tFDL_A2B: 0.1498\tFDL_B2A: 0.3904\tCL_A: 0.1836\tCL_B: 0.1389\tID_B2A: 0.2600\tID_A2B: 0.2643\tLoss_D_A: 0.1132\tLoss_D_A: 1.6499\n",
      "[2/5]\tFDL_A2B: 0.2918\tFDL_B2A: 0.2396\tCL_A: 0.1824\tCL_B: 0.1250\tID_B2A: 0.2521\tID_A2B: 0.2617\tLoss_D_A: 0.5561\tLoss_D_A: 0.4515\n",
      "[2/5]\tFDL_A2B: 0.2505\tFDL_B2A: 0.4561\tCL_A: 0.2397\tCL_B: 0.2368\tID_B2A: 0.3832\tID_A2B: 0.4810\tLoss_D_A: 0.1586\tLoss_D_A: 0.5684\n",
      "[2/5]\tFDL_A2B: 0.4484\tFDL_B2A: 0.2334\tCL_A: 0.2428\tCL_B: 0.1410\tID_B2A: 0.4742\tID_A2B: 0.2465\tLoss_D_A: 0.1124\tLoss_D_A: 0.0972\n",
      "[3/5]\tFDL_A2B: 0.3711\tFDL_B2A: 0.5171\tCL_A: 0.1693\tCL_B: 0.3326\tID_B2A: 0.2482\tID_A2B: 0.6692\tLoss_D_A: 0.1633\tLoss_D_A: 0.3719\n",
      "[3/5]\tFDL_A2B: 0.1503\tFDL_B2A: 0.3468\tCL_A: 0.1964\tCL_B: 0.1288\tID_B2A: 0.3094\tID_A2B: 0.2627\tLoss_D_A: 0.0856\tLoss_D_A: 0.1351\n",
      "[3/5]\tFDL_A2B: 0.1973\tFDL_B2A: 0.3063\tCL_A: 0.1826\tCL_B: 0.1424\tID_B2A: 0.2689\tID_A2B: 0.2648\tLoss_D_A: 0.5521\tLoss_D_A: 0.4601\n",
      "[3/5]\tFDL_A2B: 0.3597\tFDL_B2A: 0.3625\tCL_A: 0.2177\tCL_B: 0.1736\tID_B2A: 0.3687\tID_A2B: 0.2876\tLoss_D_A: 0.3374\tLoss_D_A: 0.2201\n",
      "[3/5]\tFDL_A2B: 0.2851\tFDL_B2A: 0.2156\tCL_A: 0.1912\tCL_B: 0.1885\tID_B2A: 0.2607\tID_A2B: 0.3828\tLoss_D_A: 0.1896\tLoss_D_A: 0.1174\n",
      "[3/5]\tFDL_A2B: 0.2600\tFDL_B2A: 0.2573\tCL_A: 0.1772\tCL_B: 0.1485\tID_B2A: 0.3086\tID_A2B: 0.2838\tLoss_D_A: 0.4453\tLoss_D_A: 0.6418\n",
      "[3/5]\tFDL_A2B: 0.3906\tFDL_B2A: 0.2670\tCL_A: 0.1689\tCL_B: 0.1247\tID_B2A: 0.2435\tID_A2B: 0.2588\tLoss_D_A: 0.3962\tLoss_D_A: 0.2971\n",
      "[3/5]\tFDL_A2B: 0.3983\tFDL_B2A: 0.2784\tCL_A: 0.1741\tCL_B: 0.2295\tID_B2A: 0.2739\tID_A2B: 0.4545\tLoss_D_A: 0.2261\tLoss_D_A: 0.1665\n",
      "[4/5]\tFDL_A2B: 0.2254\tFDL_B2A: 0.3125\tCL_A: 0.2088\tCL_B: 0.2230\tID_B2A: 0.3480\tID_A2B: 0.4460\tLoss_D_A: 0.3745\tLoss_D_A: 0.4724\n",
      "[4/5]\tFDL_A2B: 0.3248\tFDL_B2A: 0.2545\tCL_A: 0.2073\tCL_B: 0.1882\tID_B2A: 0.3530\tID_A2B: 0.3467\tLoss_D_A: 0.1513\tLoss_D_A: 0.0893\n",
      "[4/5]\tFDL_A2B: 0.4526\tFDL_B2A: 0.4737\tCL_A: 0.1986\tCL_B: 0.1415\tID_B2A: 0.3172\tID_A2B: 0.2438\tLoss_D_A: 0.2348\tLoss_D_A: 0.2819\n",
      "[4/5]\tFDL_A2B: 0.3823\tFDL_B2A: 0.2808\tCL_A: 0.2188\tCL_B: 0.1723\tID_B2A: 0.3883\tID_A2B: 0.2743\tLoss_D_A: 0.4201\tLoss_D_A: 0.2588\n",
      "[4/5]\tFDL_A2B: 0.2455\tFDL_B2A: 0.2741\tCL_A: 0.3060\tCL_B: 0.3001\tID_B2A: 0.5938\tID_A2B: 0.4672\tLoss_D_A: 0.1125\tLoss_D_A: 0.0562\n",
      "[4/5]\tFDL_A2B: 0.3428\tFDL_B2A: 0.6907\tCL_A: 0.2644\tCL_B: 0.1546\tID_B2A: 0.4267\tID_A2B: 0.2985\tLoss_D_A: 0.2227\tLoss_D_A: 0.2549\n",
      "[4/5]\tFDL_A2B: 0.2140\tFDL_B2A: 0.4511\tCL_A: 0.2927\tCL_B: 0.1908\tID_B2A: 0.5358\tID_A2B: 0.4211\tLoss_D_A: 0.2137\tLoss_D_A: 0.6849\n",
      "[4/5]\tFDL_A2B: 0.6196\tFDL_B2A: 0.3532\tCL_A: 0.1891\tCL_B: 0.1483\tID_B2A: 0.3878\tID_A2B: 0.3123\tLoss_D_A: 0.1132\tLoss_D_A: 0.0723\n",
      "[5/5]\tFDL_A2B: 0.2962\tFDL_B2A: 0.4215\tCL_A: 0.1995\tCL_B: 0.1743\tID_B2A: 0.3848\tID_A2B: 0.3593\tLoss_D_A: 0.3464\tLoss_D_A: 0.3800\n",
      "[5/5]\tFDL_A2B: 0.3963\tFDL_B2A: 0.3253\tCL_A: 0.2508\tCL_B: 0.3262\tID_B2A: 0.5536\tID_A2B: 0.5919\tLoss_D_A: 0.0648\tLoss_D_A: 0.0349\n",
      "[5/5]\tFDL_A2B: 0.1894\tFDL_B2A: 0.3563\tCL_A: 0.4272\tCL_B: 0.1752\tID_B2A: 0.8187\tID_A2B: 0.3542\tLoss_D_A: 0.5203\tLoss_D_A: 0.3993\n",
      "[5/5]\tFDL_A2B: 0.2713\tFDL_B2A: 0.4618\tCL_A: 0.2224\tCL_B: 0.3532\tID_B2A: 0.4434\tID_A2B: 0.7043\tLoss_D_A: 0.2656\tLoss_D_A: 0.5411\n",
      "[5/5]\tFDL_A2B: 0.3779\tFDL_B2A: 0.6181\tCL_A: 0.2018\tCL_B: 0.1515\tID_B2A: 0.4383\tID_A2B: 0.2456\tLoss_D_A: 0.1250\tLoss_D_A: 0.2855\n",
      "[5/5]\tFDL_A2B: 0.2112\tFDL_B2A: 0.4864\tCL_A: 0.2605\tCL_B: 0.1899\tID_B2A: 0.5629\tID_A2B: 0.3376\tLoss_D_A: 0.2744\tLoss_D_A: 0.5129\n",
      "[5/5]\tFDL_A2B: 0.4176\tFDL_B2A: 0.5221\tCL_A: 0.2611\tCL_B: 0.1766\tID_B2A: 0.4989\tID_A2B: 0.3562\tLoss_D_A: 0.2178\tLoss_D_A: 0.4386\n",
      "[5/5]\tFDL_A2B: 0.4834\tFDL_B2A: 0.6420\tCL_A: 0.2743\tCL_B: 0.1802\tID_B2A: 0.5783\tID_A2B: 0.3727\tLoss_D_A: 0.2025\tLoss_D_A: 0.4895\n"
     ]
    }
   ],
   "source": [
    "name = \"5res0\"\n",
    "p = Parameters(bs=4, n_channels=1, ngf=64, ndf=64, size=64, gen_n_down=2, gen_n_blocks=0, dis_n_down=2, lr=0.0002, beta1=0.9,beta1_disc=0.9)\n",
    "wandb.init(project=\"mri2sos\", entity=\"powerr\",name=name,config = {\n",
    "         \"ngf\" : p.ngf,\n",
    "         \"ndf\" : p.ndf,\n",
    "         \"gen_n_down\" : p.gen_n_down,\n",
    "         \"gen_n_blocks\" : p.gen_n_blocks,\n",
    "         \"dis_n_down\" : p.dis_n_down,\n",
    "         \"lr\" : p.lr,\n",
    "         \"beta1\" : p.beta1,\n",
    "         \"beta1_disc\" : p.beta1_disc\n",
    "        \n",
    "         })\n",
    "    \n",
    "sweep(p,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Xiaowei\\Desktop\\Clara\\CycleGAN\\Datasets\\CT_dataset -y3_ Copy'\n",
    "dataset_mri = torch.load(path+'\\A')\n",
    "dataset_sos = torch.load(path+'\\B')\n",
    "print(len(dataset_mri))\n",
    "test_mris = dataset_mri[:50][:,:,:,0]\n",
    "test_soss = dataset_sos[:50][:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for s in range(len(ps)):\n",
    "    test_fake_mris = []\n",
    "    test_fake_soss = []\n",
    "    test_rec_mris = []\n",
    "    test_rec_soss = []\n",
    "    G_A2B, G_B2A, D_A, D_B=load_models(name[s])\n",
    "    G_A2B.to(device)\n",
    "    G_B2A.to(device)\n",
    "    D_A.to(device)\n",
    "    D_B.to(device)\n",
    "    for i in range(len(test_mris)):\n",
    "        real_mri = torch.Tensor(test_mris[i]).unsqueeze(0).unsqueeze(0)\n",
    "        fake_sos = G_A2B(real_mri.to(device)).cpu().detach().numpy()\n",
    "        rec_mri = G_B2A(torch.Tensor(fake_sos).to(device)).cpu().detach().numpy()\n",
    "\n",
    "        real_sos = torch.Tensor(test_soss[i]).unsqueeze(0).unsqueeze(0)\n",
    "        fake_mri = G_B2A(real_sos.to(device)).cpu().detach().numpy()\n",
    "        rec_sos = G_A2B(torch.Tensor(fake_mri).to(device)).cpu().detach().numpy()\n",
    "\n",
    "        test_fake_mris.append(fake_mri)\n",
    "        test_fake_soss.append(fake_sos)\n",
    "        test_rec_mris.append(rec_mri)\n",
    "        test_rec_soss.append(rec_sos)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            real_mri.detach().cpu()\n",
    "            real_sos.detach().cpu()\n",
    "    n=3\n",
    "    print(name[s])\n",
    "    plt.subplots(2,4,figsize=(16,6));\n",
    "    plt.subplot(2,4,1);plt.imshow(test_mris[n]); plt.title('Real MRI'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "    plt.subplot(2,4,2);plt.imshow(test_fake_mris[n][0,0,:,:]); plt.title('Artificial MRI'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "    plt.subplot(2,4,3);plt.imshow(test_rec_mris[n][0,0,:,:]); plt.title('Rec. MRI'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "    plt.subplot(2,4,4);plt.imshow(test_mris[n]-test_fake_mris[n][0,0,:,:], cmap='PiYG');plt.colorbar();plt.title('Error'); plt.clim(-0.3,0.3); plt.axis('off')\n",
    "    plt.subplot(2,4,5);plt.imshow(test_soss[n]); plt.title('Real CT'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "    plt.subplot(2,4,6);plt.imshow(test_fake_soss[n][0,0,:,:]); plt.title('Artificial CT'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "    plt.subplot(2,4,7);plt.imshow(test_rec_soss[n][0,0,:,:]); plt.title('Rec. CT'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "    plt.subplot(2,4,8);plt.imshow(test_soss[n]-test_fake_soss[n][0,0,:,:], cmap='PiYG');plt.colorbar();plt.title('Error'); plt.clim(-0.3,0.3); plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fake_mris = []\n",
    "test_fake_soss = []\n",
    "test_rec_mris = []\n",
    "test_rec_soss = []\n",
    "G_A2B, G_B2A, D_A, D_B=load_models(\"bs4_train3\")\n",
    "G_A2B.to(device)\n",
    "G_B2A.to(device)\n",
    "D_A.to(device)\n",
    "D_B.to(device)\n",
    "for i in range(len(test_mris)):\n",
    "        real_mri = torch.Tensor(test_mris[i]).unsqueeze(0).unsqueeze(0)\n",
    "        fake_sos = G_A2B(real_mri.to(device)).cpu().detach().numpy()\n",
    "        rec_mri = G_B2A(torch.Tensor(fake_sos).to(device)).cpu().detach().numpy()\n",
    "\n",
    "        real_sos = torch.Tensor(test_soss[i]).unsqueeze(0).unsqueeze(0)\n",
    "        fake_mri = G_B2A(real_sos.to(device)).cpu().detach().numpy()\n",
    "        rec_sos = G_A2B(torch.Tensor(fake_mri).to(device)).cpu().detach().numpy()\n",
    "\n",
    "        test_fake_mris.append(fake_mri)\n",
    "        test_fake_soss.append(fake_sos)\n",
    "        test_rec_mris.append(rec_mri)\n",
    "        test_rec_soss.append(rec_sos)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            real_mri.detach().cpu()\n",
    "            real_sos.detach().cpu()\n",
    "n=3\n",
    "plt.subplots(2,4,figsize=(16,6));\n",
    "plt.subplot(2,4,1);plt.imshow(test_mris[n]); plt.title('Real MRI'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "plt.subplot(2,4,2);plt.imshow(test_fake_mris[n][0,0,:,:]); plt.title('Artificial MRI'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "plt.subplot(2,4,3);plt.imshow(test_rec_mris[n][0,0,:,:]); plt.title('Rec. MRI'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "plt.subplot(2,4,4);plt.imshow(test_mris[n]-test_fake_mris[n][0,0,:,:], cmap='PiYG');plt.colorbar();plt.title('Error'); plt.clim(-0.3,0.3); plt.axis('off')\n",
    "plt.subplot(2,4,5);plt.imshow(test_soss[n]); plt.title('Real SoS'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "plt.subplot(2,4,6);plt.imshow(test_fake_soss[n][0,0,:,:]); plt.title('Artificial SoS'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "plt.subplot(2,4,7);plt.imshow(test_rec_soss[n][0,0,:,:]); plt.title('Rec. SoS'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "plt.subplot(2,4,8);plt.imshow(test_soss[n]-test_fake_soss[n][0,0,:,:], cmap='PiYG');plt.colorbar();plt.title('Error'); plt.clim(-0.3,0.3); plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ps[2]\n",
    "name=\"bs4_train3\"\n",
    "G_A2B, G_B2A, D_A, D_B=load_models(\"bs4_train2\")\n",
    "G_A2B.to(device)\n",
    "G_B2A.to(device)\n",
    "D_A.to(device)\n",
    "D_B.to(device)\n",
    "dataloader_mri, dataloader_sos = init_dataloaders(p)\n",
    "optimizer_G_A2B = torch.optim.Adam(G_A2B.parameters(), lr=p.lr, betas=(p.beta1, 0.999))\n",
    "optimizer_G_B2A = torch.optim.Adam(G_B2A.parameters(), lr=p.lr, betas=(p.beta1, 0.999))\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=p.lr, betas=(p.beta1_disc, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=p.lr, betas=(p.beta1_disc, 0.999))\n",
    "wandb.init(project=\"mri2sos\", entity=\"powerr\", name=name,config = {\n",
    "         \"ngf\" : p.ngf,\n",
    "         \"ndf\" : p.ndf,\n",
    "         \"gen_n_down\" : p.gen_n_down,\n",
    "         \"gen_n_blocks\" : p.gen_n_blocks,\n",
    "         \"dis_n_down\" : p.dis_n_down,\n",
    "         \"lr\" : p.lr,\n",
    "         \"beta1\" : p.beta1,\n",
    "         \"beta1_disc\" : p.beta1_disc\n",
    "        \n",
    "         })\n",
    "G_losses, D_losses = training(10, dataloader_mri, dataloader_sos, G_A2B, G_B2A, D_A, D_B, optimizer_G_A2B, optimizer_G_B2A, optimizer_D_A, optimizer_D_B,name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_validation(bs):\n",
    "    workers = 2\n",
    "    image_size = (64,64)\n",
    "    dataroot = r'C:\\Users\\Xiaowei\\Desktop\\Clara\\CycleGAN\\Datasets\\same_patch_dataset\\validation'\n",
    "    datasets_val = dset.ImageFolder(root=dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(image_size),\n",
    "                                   transforms.CenterCrop(image_size),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0, 0, 0), (1, 1, 1)),\n",
    "                                  ]))\n",
    "\n",
    "    from torch.utils import data\n",
    "    idx = [i for i in range(len(datasets_val)) if datasets_val.imgs[i][1] != datasets_val.class_to_idx['B']]\n",
    "    mri_subset = data.Subset(datasets_val, idx)\n",
    "    dataloader_mri = torch.utils.data.DataLoader(mri_subset, batch_size=bs,\n",
    "                                             shuffle=True, num_workers=workers)\n",
    "    idx = [i for i in range(len(datasets_val)) if datasets_val.imgs[i][1] != datasets_val.class_to_idx['A']]\n",
    "    sos_subset = data.Subset(datasets_val, idx)\n",
    "\n",
    "    dataloader_sos = torch.utils.data.DataLoader(sos_subset, batch_size=bs,\n",
    "                                             shuffle=True, num_workers=workers)\n",
    "    return dataloader_mri, dataloader_sos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_validation(dataloader_mri, dataloader_sos,G_A2B, G_B2A, D_A, D_B):\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    for i,(data_mri, data_sos) in enumerate(zip(dataloader_mri, dataloader_sos),0):\n",
    "                D_loss = 0\n",
    "                # Set model input\n",
    "                a_real = data_mri[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "                b_real = data_sos[0][:,0,:,:].unsqueeze(1).to(device)\n",
    "\n",
    "                # Generated images\n",
    "                # print(\"a_real shape: \"+str(a_real.shape))\n",
    "                b_fake = G_A2B(a_real)\n",
    "                # print(\"fake image generated\")\n",
    "                a_rec = G_B2A(b_fake)\n",
    "                a_fake = G_B2A(b_real)\n",
    "                b_rec = G_A2B(a_fake)\n",
    "\n",
    "                # CALCULATE DISCRIMINATORS LOSSES\n",
    "                # Discriminator A\n",
    "                Disc_loss_A = LSGAN_D(D_A(a_real), D_A(a_fake.detach()))\n",
    "                D_loss += Disc_loss_A.item()\n",
    "                # Discriminator B\n",
    "                Disc_loss_B =  LSGAN_D(D_B(b_real), D_B(b_fake.detach()))\n",
    "                D_loss += Disc_loss_B.item()\n",
    "                D_losses.append(D_loss) \n",
    "                \n",
    "                # Generator\n",
    "                # CALCULATE GENERATORS LOSSES\n",
    "                Fool_disc_loss_A2B = LSGAN_G(D_B(b_fake))\n",
    "                Fool_disc_loss_B2A = LSGAN_G(D_A(a_fake))\n",
    "\n",
    "                # Cycle Consistency    both use the two generators\n",
    "                Cycle_loss_A = criterion_Im(a_rec, a_real)*5\n",
    "                Cycle_loss_B = criterion_Im(b_rec, b_real)*5\n",
    "\n",
    "                # Identity loss\n",
    "                Id_loss_B2A = criterion_Im(G_B2A(a_real), a_real)*10\n",
    "                Id_loss_A2B = criterion_Im(G_A2B(b_real), b_real)*10\n",
    "\n",
    "                # generator losses\n",
    "                Loss_G = Fool_disc_loss_A2B+Fool_disc_loss_B2A+Cycle_loss_A+Cycle_loss_B+Id_loss_B2A+Id_loss_A2B\n",
    "    \n",
    "                G_losses.append(Loss_G.detach().cpu().numpy())\n",
    "\n",
    "                del a_real, b_real, a_fake, b_fake            \n",
    "    avg_gen_loss = np.mean(G_losses)\n",
    "    avg_disc_loss = np.mean(D_losses)\n",
    "    std_gen_loss = np.std(G_losses)\n",
    "    std_disc_loss = np.std(D_losses)\n",
    "    print('avg_gen_loss: %.4f\\tstd_gen_loss: %.4f\\tavg_disc_loss: %.4f\\tstd_disc_loss: %.4f'\n",
    "                                % ( avg_gen_loss,std_gen_loss,avg_disc_loss,std_disc_loss))\n",
    "    #print('avg_gen_loss: %.4f\\tavg_disc_loss: %.4f'\n",
    "    #                            % ( avg_gen_loss,avg_disc_loss))\n",
    "    return Loss_G, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_gen_loss: 3.9414\tstd_gen_loss: 2.7368\tavg_disc_loss: 1.1460\tstd_disc_loss: 0.4365\n",
      "avg_gen_loss: 3.6166\tstd_gen_loss: 2.7162\tavg_disc_loss: 1.1550\tstd_disc_loss: 0.3363\n",
      "avg_gen_loss: 3.6983\tstd_gen_loss: 2.8373\tavg_disc_loss: 1.1426\tstd_disc_loss: 0.3172\n",
      "avg_gen_loss: 3.5958\tstd_gen_loss: 2.9128\tavg_disc_loss: 1.2081\tstd_disc_loss: 0.2677\n",
      "avg_gen_loss: 3.3787\tstd_gen_loss: 2.8770\tavg_disc_loss: 1.1792\tstd_disc_loss: 0.1993\n",
      "avg_gen_loss: 3.9690\tstd_gen_loss: 2.6416\tavg_disc_loss: 0.9278\tstd_disc_loss: 0.3077\n",
      "avg_gen_loss: 3.8605\tstd_gen_loss: 2.8516\tavg_disc_loss: 1.0614\tstd_disc_loss: 0.2826\n",
      "avg_gen_loss: 4.0402\tstd_gen_loss: 2.8685\tavg_disc_loss: 1.0228\tstd_disc_loss: 0.2112\n",
      "avg_gen_loss: 3.7972\tstd_gen_loss: 2.8988\tavg_disc_loss: 1.0978\tstd_disc_loss: 0.2972\n",
      "avg_gen_loss: 3.6533\tstd_gen_loss: 2.8699\tavg_disc_loss: 1.0550\tstd_disc_loss: 0.1861\n",
      "avg_gen_loss: 3.9947\tstd_gen_loss: 2.6650\tavg_disc_loss: 0.9081\tstd_disc_loss: 0.2885\n",
      "avg_gen_loss: 3.7623\tstd_gen_loss: 2.7075\tavg_disc_loss: 1.0613\tstd_disc_loss: 0.1912\n",
      "avg_gen_loss: 3.7772\tstd_gen_loss: 2.8788\tavg_disc_loss: 1.0327\tstd_disc_loss: 0.1901\n",
      "avg_gen_loss: 3.7453\tstd_gen_loss: 2.8917\tavg_disc_loss: 1.3001\tstd_disc_loss: 0.2894\n",
      "avg_gen_loss: 3.5507\tstd_gen_loss: 2.8415\tavg_disc_loss: 1.0352\tstd_disc_loss: 0.1984\n",
      "avg_gen_loss: 4.3032\tstd_gen_loss: 2.7876\tavg_disc_loss: 0.8136\tstd_disc_loss: 0.2270\n",
      "avg_gen_loss: 3.6113\tstd_gen_loss: 2.7476\tavg_disc_loss: 1.1443\tstd_disc_loss: 0.3083\n",
      "avg_gen_loss: 3.9699\tstd_gen_loss: 2.7966\tavg_disc_loss: 1.1586\tstd_disc_loss: 0.3406\n",
      "avg_gen_loss: 3.5818\tstd_gen_loss: 2.8687\tavg_disc_loss: 1.2365\tstd_disc_loss: 0.2775\n",
      "avg_gen_loss: 3.5784\tstd_gen_loss: 2.8986\tavg_disc_loss: 1.1558\tstd_disc_loss: 0.2130\n"
     ]
    }
   ],
   "source": [
    "a_validation,b_validation = dataloader_validation(1)\n",
    "#name = ['0resnet00', '1resnet00', '2resnet00', '3resnet00', '4resnet00', '0resnet10', '1resnet10', '2resnet10', '3resnet10', '4resnet10', '0resnet20', '1resnet20', '2resnet20', '3resnet20', '4resnet20', '0resnet30', '1resnet30', '2resnet30', '3resnet30', '4resnet30', '0resnet40', '1resnet40', '2resnet40', '3resnet40', '4resnet40', '0resnet50', '1resnet50', '2resnet50', '3resnet50', '4resnet50', '0resnet60', '1resnet60', '2resnet60', '3resnet60', '4resnet60', '0resnet70', '1resnet70', '2resnet70', '3resnet70', '4resnet70', '0resnet80', '1resnet80', '2resnet80', '3resnet80', '4resnet80', '0resnet90', '1resnet90', '2resnet90', '3resnet90', '4resnet90']\n",
    "name=[,'0gener10', '1gener10', '2gener10', '3gener10', '4gener10','0gener11', '1gener11', '2gener11', '3gener11', '4gener11','0gener12', '1gener12', '2gener12', '3gener12', '4gener12']\n",
    "n=0\n",
    "for model in range(len(name)):\n",
    "    G_A2B, G_B2A, D_A, D_B=load_models(name[model])\n",
    "    G_A2B.to(device)\n",
    "    G_B2A.to(device)\n",
    "    D_A.to(device)\n",
    "    D_B.to(device)\n",
    "    test_validation(a_validation,b_validation,G_A2B, G_B2A, D_A, D_B)\n",
    "#     real_mri = torch.Tensor(a_validation[n]).unsqueeze(0).unsqueeze(0)\n",
    "#     fake_sos = G_A2B(real_mri.to(device)).cpu().detach().numpy()\n",
    "#     rec_mri = G_B2A(torch.Tensor(fake_sos).to(device)).cpu().detach().numpy()\n",
    "\n",
    "#     real_sos = torch.Tensor(b_validation[n]).unsqueeze(0).unsqueeze(0)\n",
    "#     fake_mri = G_B2A(real_sos.to(device)).cpu().detach().numpy()\n",
    "#     rec_sos = G_A2B(torch.Tensor(fake_mri).to(device)).cpu().detach().numpy()\n",
    "\n",
    "#     test_fake_mris.append(fake_mri)\n",
    "#     test_fake_soss.append(fake_sos)\n",
    "#     test_rec_mris.append(rec_mri)\n",
    "#     test_rec_soss.append(rec_sos)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#             real_mri.detach().cpu()\n",
    "#             real_sos.detach().cpu()\n",
    "#     print(name[s])\n",
    "#     plt.subplots(2,4,figsize=(16,6));\n",
    "#     plt.subplot(2,4,1);plt.imshow(a_validation[n]); plt.title('Real MRI'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "#     plt.subplot(2,4,2);plt.imshow(test_fake_mris[n][0,0,:,:]); plt.title('Artificial MRI'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "#     plt.subplot(2,4,3);plt.imshow(test_rec_mris[n][0,0,:,:]); plt.title('Rec. MRI'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "#     plt.subplot(2,4,4);plt.imshow(a_validation[n]-test_fake_mris[n][0,0,:,:], cmap='PiYG');plt.colorbar();plt.title('Error'); plt.clim(-0.3,0.3); plt.axis('off')\n",
    "#     plt.subplot(2,4,5);plt.imshow(b_validation[n]); plt.title('Real SoS'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "#     plt.subplot(2,4,6);plt.imshow(test_fake_soss[n][0,0,:,:]); plt.title('Artificial SoS'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "#     plt.subplot(2,4,7);plt.imshow(test_rec_soss[n][0,0,:,:]); plt.title('Rec. SoS'); plt.colorbar(); plt.clim(0,1); plt.axis('off')\n",
    "#     plt.subplot(2,4,8);plt.imshow(b_validation[n]-test_fake_soss[n][0,0,:,:], cmap='PiYG');plt.colorbar();plt.title('Error'); plt.clim(-0.3,0.3); plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0aaea334e830e340cd4f52dc73b047db0349b898aecbb4e3ab01bb688be8ea9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
